#!/bin/bash -l
#SBATCH -C gpu
#SBATCH -t 00:05:00 
#SBATCH -J AMREX_GPU
#SBATCH -o AMREX_GPU.o%j
#SBATCH -A nstaff 

#SBATCH -N 2
#SBATCH -n 16 
#SBATCH -c 10
#SBATCH --gres=gpu:8
#SBATCH --ntasks-per-node=8

# Note: Given exclusive configuration mode,
#       you MUST specify your desired resources up top like this.
#       Cannot put it in the srun line alone.
#       (You can force lower than your full request in the srun line,
#        or put the configuration again for safety, but shouldn't be needed.)
# ============
# -N =                nodes
# -n =                tasks (MPI ranks)
# -c =                CPU per task (full coriGPU node, c*n <= 80)
# --gres=gpu: =       GPUs per node (full coriGPU node, 8)
# --ntasks-per-node = number of tasks (MPI ranks) per node (full node, 8)
#

# For one node:  -N 1, -n  8, -c 10, --gres=gpu:8 --ntasks-per-node 8
# For two nodes: -N 2, -n 16, -c 10, --gres=gpu:8 --ntasks-per-node 8

# salloc commands:
# ================
# Single GPU. (If you don't require an independent node, please use a shared node.)
# Request 30GB per GPU being requested.
# salloc -N 1 -t 2:00:00 -c 10 -C gpu --mem=30GB --gres=gpu:1 -A m1759
# Single node:
# salloc -N 1 -t 2:00:00 -c 80 -C gpu --exclusive --gres=gpu:8 -A m1759 
# Multi node:
# salloc -N 2 -t 2:00:00 -c 80 -C gpu --exclusive --gres=gpu:8 -A m1759

# environment setup:
# ==================
# module purge
# module load modules esslurm gcc cuda mvapich2 
#    or for openmpi:
# module load modules esslurm gcc cuda openmpi/3.1.0-ucx


EXE=./CNS3d.gnu.TPROF.MPI.CUDA.ex
INPUTS=inputs

# Run inside the current salloc session using available resources.
# Change parameters to match available resources & run with "./run.corigpu"
# srun -n 8 -c 10 --gres=gpu:8 ${EXE} ${INPUTS}

# Submit with the SBATCH configuration above to the gpu queue: "sbatch run.corigpu"
# Can also be ran with "./run.corigpu" to run with 1 CPU and 1 GPU.
srun ${EXE} ${INPUTS}
