[main]
testTopDir = .
webTopDir  = ./web

MAKE = make
sourceTree = AMReX
numMakeJobs = 32

COMP = gcc
add_to_c_make_command = TEST=TRUE USE_ASSERTION=TRUE

purge_output = 1

# suiteName is the name prepended to all output directories
suiteName = AMReX-SYCL

reportActiveTestsOnly = 1

# Add "GO UP" link at the top of the web page?
goUpLink = 1

# email
sendEmailWhenFail = 0
#emailTo = # jane.doe@example.com
emailTo = 
#emailBody = #Check https://ccse.lbl.gov/pub/GpuRegressionTesting/AMReX/ for more details.
emailBody =

# MPIcommand should use the placeholders:
#   @host@ to indicate where to put the hostname to run on
#   @nprocs@ to indicate where to put the number of processors
#   @command@ to indicate where to put the command to run
#
# only tests with useMPI = 1 will run in parallel
# nprocs is problem dependent and specified in the individual problem
# sections.

#MPIcommand = mpiexec -host @host@ -n @nprocs@ @command@
MPIcommand = mpiexec -n @nprocs@ @command@
MPIhost = 

[AMReX]
dir = ../../mygitrepo/amrex/
branch = "development"

# individual problems follow

[CNS-RT]
addToCompileString = USE_CUDA=FALSE USE_HIP=FALSE USE_SYCL=TRUE
buildDir = Tests/GPU/CNS/Exec/RT
inputFile = inputs-rt
dim = 3
restartTest = 0
useMPI = 0
numprocs = 4
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
testSrcTree = C_Src

[CNS-Sod]
addToCompileString = USE_CUDA=FALSE USE_SYCL=TRUE
buildDir = Tests/GPU/CNS/Exec/Sod
inputFile = inputs-rt
dim = 3
restartTest = 0
useMPI = 0
numprocs = 2
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
testSrcTree = C_Src

[EB_Cell_Dir_2D]
addToCompileString = USE_CUDA=FALSE USE_HIP=FALSE USE_SYCL=TRUE
tolerance = 2.e-12
runtime_params = gpu_regtest=1
buildDir = Tests/LinearSolvers/CellEB2
inputFile = inputs.rt.2d
dim = 2
restartTest = 0
useMPI = 0
numprocs = 2
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

[EB_Cell_Dir_3D]
addToCompileString = USE_CUDA=FALSE USE_SYCL=TRUE
tolerance = 2.e-11
runtime_params = eb2.geom_type=rotated_box gpu_regtest=1
buildDir = Tests/LinearSolvers/CellEB2
inputFile = inputs.rt.3d
dim = 3
restartTest = 0
useMPI = 0
numprocs = 4
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

[EB_Cell_Neu_2D]
addToCompileString = USE_CUDA=FALSE USE_SYCL=TRUE
tolerance = 7.e-12
buildDir = Tests/LinearSolvers/CellEB
inputFile = inputs.rt.2d
dim = 2
restartTest = 0
useMPI = 0
numprocs = 2
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

[EB_Cell_Neu_3D]
addToCompileString = USE_CUDA=FALSE USE_SYCL=TRUE
tolerance = 3.e-12
buildDir = Tests/LinearSolvers/CellEB
inputFile = inputs.rt.3d
dim = 3
restartTest = 0
useMPI = 0
numprocs = 4
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

[EB_Node_2D]
addToCompileString = USE_CUDA=FALSE USE_SYCL=TRUE
tolerance = 1.e-12
buildDir = Tests/LinearSolvers/NodeEB
inputFile = inputs.rt.2d
dim = 2
restartTest = 0
useMPI = 0
numprocs = 4
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

[EB_Node_3D]
addToCompileString = USE_CUDA=FALSE USE_HIP=FALSE USE_SYCL=TRUE
tolerance = 1.e-11
runtime_params = gpu_regtest=1
buildDir = Tests/LinearSolvers/NodeEB
inputFile = inputs.rt.3d.x
dim = 3
restartTest = 0
useMPI = 0
numprocs = 4
useOMP = 0
numthreads = 1
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

[EB_Tensor_3D]
addToCompileString = USE_CUDA=FALSE USE_HIP=FALSE USE_SYCL=TRUE
tolerance = 6.e-10
runtime_params = gpu_regtest=1
buildDir = Tests/LinearSolvers/EBTensor
inputFile = inputs.rt.3d
dim = 3
restartTest = 0
useMPI = 0
numprocs = 4
useOMP = 0
numthreads = 1
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

[MLMG_ABecCom]
addToCompileString = USE_CUDA=FALSE USE_HIP=FALSE USE_SYCL=TRUE
tolerance = 1.e-13
runtime_params = gpu_regtest=1
buildDir = Tests/LinearSolvers/ABecLaplacian_C
inputFile = inputs-rt-abeclap-com
dim = 3
restartTest = 0
useMPI = 0
numprocs = 4
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

[MLMG_NodalPoisson]
addToCompileString = USE_CUDA=FALSE USE_HIP=FALSE USE_SYCL=TRUE
tolerance = 3.e-11
runtime_params = gpu_regtest=1
buildDir = Tests/LinearSolvers/NodalPoisson
inputFile = inputs-rt
dim = 3
restartTest = 0
useMPI = 0
numprocs = 4
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

[MLMG_PoisLev] 
addToCompileString = USE_CUDA=FALSE USE_HIP=FALSE USE_SYCL=TRUE
tolerance = 1.e-12
buildDir = Tests/LinearSolvers/ABecLaplacian_C
inputFile = inputs-rt-poisson-lev
dim = 3
restartTest = 0
useMPI = 0
numprocs = 2
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
outputFile = plot
testSrcTree = C_Src

### The asyncout fails
[NeighborParticles]
addToCompileString = USE_CUDA=FALSE USE_SYCL=TRUE
buildDir = Tests/Particles/NeighborParticles
inputFile = inputs
runtime_params = amrex.async_out=0
dim = 3
restartTest = 0
useMPI = 0
numprocs = 1
useOMP = 0
compileTest = 0
doComparison = 1
diffDir = neighbor_test.0
selfTest = 0
doVis = 0
testSrcTree = C_Src

[ParticleMesh]
addToCompileString = USE_CUDA=FALSE USE_HIP=FALSE USE_SYCL=TRUE
tolerance = 5.e-14
buildDir = Tests/Particles/ParticleMesh
runtime_params = nx=64 ny=64 nz=64
inputFile = inputs
dim = 3
restartTest = 0
useMPI = 0
numprocs = 1
useOMP = 0
numthreads = 2
compileTest = 0
doVis = 0
outputFile = plot
compareParticles = 1
particleTypes = particle0
testSrcTree = C_Src

[ParticleReduce]
addToCompileString = USE_CUDA=FALSE USE_HIP=FALSE USE_SYCL=TRUE
buildDir = Tests/Particles/ParticleReduce
inputFile = inputs
dim = 3
restartTest = 0
useMPI = 0
numprocs = 1
useOMP = 0
compileTest = 0
selfTest = 1
stSuccessString = pass
doVis = 0
testSrcTree = C_Src

# Used to fail from time to time
[ParticleTransformation]
addToCompileString = USE_CUDA=FALSE USE_SYCL=TRUE
buildDir = Tests/Particles/ParticleTransformations
inputFile = inputs
dim = 3
restartTest = 0
useMPI = 0
numprocs = 1
useOMP = 0
compileTest = 0
selfTest = 1
stSuccessString = pass
doVis = 0
testSrcTree = C_Src

[Redistribute]
addToCompileString = USE_CUDA=FALSE USE_HIP=FALSE USE_SYCL=TRUE
buildDir = Tests/Particles/Redistribute
inputFile = inputs.rt.cuda
runtime_params = redistribute.do_random=0
dim = 3
restartTest = 0
useMPI = 0
numprocs = 2
useOMP = 0
compileTest = 0
selfTest = 1
stSuccessString = pass
doVis = 0
testSrcTree = C_Src

[RedistributeMR]
addToCompileString = USE_CUDA=FALSE USE_SYCL=TRUE
buildDir = Tests/Particles/Redistribute
inputFile = inputs.rt.cuda.mr
dim = 3
restartTest = 0
useMPI = 0
numprocs = 2
useOMP = 0
compileTest = 0
selfTest = 1
stSuccessString = pass
doVis = 0
testSrcTree = C_Src

[RedistributeMR_2D]
addToCompileString = USE_CUDA=FALSE USE_SYCL=TRUE
buildDir = Tests/Particles/Redistribute
inputFile = inputs.rt.cuda.mr
dim = 2
restartTest = 0
useMPI = 0
numprocs = 2
useOMP = 0
compileTest = 0
selfTest = 1
stSuccessString = pass
doVis = 0
testSrcTree = C_Src

[RedistributeNonPeriodic]
addToCompileString = USE_CUDA=FALSE USE_SYCL=TRUE
buildDir = Tests/Particles/Redistribute
inputFile = inputs.rt.cuda.nonperiodic
dim = 3
restartTest = 0
useMPI = 0
numprocs = 2
useOMP = 0
compileTest = 0
selfTest = 1
stSuccessString = pass
doVis = 0
testSrcTree = C_Src

[SortParticlesByCell]
addToCompileString = USE_CUDA=FALSE USE_SYCL=TRUE
buildDir = Tests/Particles/Redistribute
inputFile = inputs.rt.cuda.sort
dim = 3
restartTest = 0
useMPI = 0
numprocs = 2
useOMP = 0
compileTest = 0
selfTest = 1
stSuccessString = pass
doVis = 0
testSrcTree = C_Src

[Tracer]
addToCompileString = USE_CUDA=FALSE USE_HIP=FALSE USE_SYCL=TRUE
buildDir = Tests/Amr/Advection_AmrLevel/Exec/SingleVortex
inputFile = inputs.tracers
runtime_params = particles.do_tiling=0
dim = 2
restartTest = 0
useMPI = 0
numprocs = 2
useOMP = 0
compileTest = 0
compareParticles = 1
doVis = 0
testSrcTree = C_Src

[Vector]
addToCompileString = USE_CUDA=FALSE USE_SYCL=TRUE
buildDir = Tests/GPU/Vector
inputFile = inputs
dim = 3
restartTest = 0
useMPI = 0
numprocs = 1
useOMP = 0
compileTest = 0
selfTest = 1
stSuccessString = Passed
doVis = 0
testSrcTree = C_Src
